{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting Started with Spark\n",
        "\n",
        "### Author - Manickashree \"Madhu\"\n",
        "\n",
        "#### 1. Setting up your Spark instance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KLOXRs_oo7q",
        "outputId": "23090a39-1f3a-4fdb-e4dc-b33604b9dbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.4.0\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark==3.4.0) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317122 sha256=58a68e0cbc569fab103e441cce01fd060575a6a27a5085bda824d0e3946c55a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "# Run below commands\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null #Install java\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz ## Install Apache Spark\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark==3.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BCWS-_Hhno2P"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gziw2IzEyoPC",
        "outputId": "69f8b8a8-8eff-4a73-a839-a299c8175c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;32mbeeline\u001b[0m*               load-spark-env.sh  \u001b[01;32mspark-class\u001b[0m*          sparkR.cmd        spark-sql.cmd\n",
            "beeline.cmd            \u001b[01;32mpyspark\u001b[0m*           \u001b[01;32mspark-class2.cmd\u001b[0m*     \u001b[01;32mspark-shell\u001b[0m*      \u001b[01;32mspark-submit\u001b[0m*\n",
            "\u001b[01;32mdocker-image-tool.sh\u001b[0m*  pyspark2.cmd       spark-class.cmd       spark-shell2.cmd  spark-submit2.cmd\n",
            "\u001b[01;32mfind-spark-home\u001b[0m*       pyspark.cmd        \u001b[01;32mspark-connect-shell\u001b[0m*  spark-shell.cmd   spark-submit.cmd\n",
            "find-spark-home.cmd    \u001b[01;32mrun-example\u001b[0m*       \u001b[01;32msparkR\u001b[0m*               \u001b[01;32mspark-sql\u001b[0m*\n",
            "load-spark-env.cmd     run-example.cmd    sparkR2.cmd           spark-sql2.cmd\n"
          ]
        }
      ],
      "source": [
        "ls /content/spark-3.4.0-bin-hadoop3/bin\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j5Oet87Goj_I"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from datetime import datetime, date, timedelta\n",
        "from dateutil import relativedelta\n",
        "from pyspark.sql import SQLContext, Row\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.functions import to_timestamp, to_date\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import collect_list, collect_set, concat, first, last, array_distinct, col, size, expr\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import isnan\n",
        "from pyspark.sql.functions import year\n",
        "from pyspark.sql.functions import max\n",
        "from pyspark.sql.functions import desc\n",
        "from pyspark.sql.functions import max, min\n",
        "import random\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0GKzFcMjnoAb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"my session\") \\\n",
        "    .getOrCreate()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_g6DE2Gxpgju"
      },
      "outputs": [],
      "source": [
        "flightData2015 = spark.read.csv(\"/content/2015-summary.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr40FwXeqc-H",
        "outputId": "7fb9be76-ed9d-47d0-9891-730e7fb4b0a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
              " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flightData2015.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDUgkPKnqdgY",
        "outputId": "05f4ad75-ba5f-454c-a8c8-5ba98e5d2935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [count#19 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(count#19 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=33]\n",
            "      +- FileScan csv [DEST_COUNTRY_NAME#17,ORIGIN_COUNTRY_NAME#18,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flightData2015.sort(\"count\").explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WwF_CqSRqgci"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqS8dETJqheB",
        "outputId": "5a7d1daf-bd94-426f-c44f-aa7bfe66e3e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
              " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flightData2015.sort(\"count\").take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HzUSj0khqkVA"
      },
      "outputs": [],
      "source": [
        "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "2JQIoV01qlsW"
      },
      "outputs": [],
      "source": [
        "sqlWay = spark.sql(\"\"\"\n",
        "  SELECT DEST_COUNTRY_NAME, count(1)\n",
        "  FROM flight_data_2015\n",
        "  GROUP BY DEST_COUNTRY_NAME\n",
        "  \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "F4SZ9JZ9qoTJ"
      },
      "outputs": [],
      "source": [
        "dataFrameWay = flightData2015\\\n",
        "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "    .count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rpBztbAqp7b",
        "outputId": "3f9db973-0c13-4b02-d3a2-eb1bbd9202ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=55]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=68]\n",
            "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_count(1)])\n",
            "         +- FileScan csv [DEST_COUNTRY_NAME#17] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sqlWay.explain()\n",
        "dataFrameWay.explain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQp7HWWyqr9B",
        "outputId": "83ff30c2-9d30-471e-be83-8ce5a2a5c280"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcYC9-llqtir",
        "outputId": "6f4a4b0f-692f-4566-8d8d-262e5616a634"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "flightData2015.select(max(\"count\")).take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKHczoS7quE6",
        "outputId": "ce66a8ee-d91f-4d42-8466-da8693b0578d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "maxSql = spark.sql(\"\"\"\n",
        "  SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
        "  FROM flight_data_2015\n",
        "  GROUP BY DEST_COUNTRY_NAME\n",
        "  ORDER BY sum(count) DESC\n",
        "  LIMIT 5\n",
        "  \"\"\")\n",
        "maxSql.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CD8WAPEqvk6",
        "outputId": "99765e60-c940-4598-dbd1-001174be9911"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|destination_total|\n",
            "+-----------------+-----------------+\n",
            "|    United States|           411352|\n",
            "|           Canada|             8399|\n",
            "|           Mexico|             7140|\n",
            "|   United Kingdom|             2025|\n",
            "|            Japan|             1548|\n",
            "+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flightData2015\\\n",
        "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "    .sum(\"count\")\\\n",
        "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        "    .sort(desc(\"destination_total\"))\\\n",
        "    .limit(5)\\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxvRzc8Iq3BM",
        "outputId": "7aa588bc-0649-4bd1-f410-8591eaa2eed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#113L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#17,destination_total#113L])\n",
            "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[sum(count#19)])\n",
            "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#17, 5), ENSURE_REQUIREMENTS, [plan_id=238]\n",
            "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#17], functions=[partial_sum(count#19)])\n",
            "            +- FileScan csv [DEST_COUNTRY_NAME#17,count#19] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/content/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "flightData2015\\\n",
        "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
        "    .sum(\"count\")\\\n",
        "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
        "    .sort(desc(\"destination_total\"))\\\n",
        "    .limit(5)\\\n",
        "    .explain()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_dvY0DQq5a6"
      },
      "source": [
        "**Climate** **Change**: **Project Tasmania**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "elbEPinUq_mx"
      },
      "outputs": [],
      "source": [
        "temp = spark.read.csv(\"/content/GlobalLandTemperatures_GlobalLandTemperaturesByCountry.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7qUUWwhrMNL",
        "outputId": "b11fdcf9-2378-4f58-9445-a1f182d2e38a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+------------------+-----------------------------+-------+\n",
            "|        dt|AverageTemperature|AverageTemperatureUncertainty|Country|\n",
            "+----------+------------------+-----------------------------+-------+\n",
            "|1743-11-01|4.3839999999999995|                        2.294|  Åland|\n",
            "|1743-12-01|              null|                         null|  Åland|\n",
            "|1744-01-01|              null|                         null|  Åland|\n",
            "|1744-02-01|              null|                         null|  Åland|\n",
            "+----------+------------------+-----------------------------+-------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "temp.show(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "T9TnYwoRrNbj"
      },
      "outputs": [],
      "source": [
        "temp.createOrReplaceTempView(\"temp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2MwdDtKrQvj"
      },
      "source": [
        "a. For which country and during what year, the highest average temperature was observed?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "6jBEpVUNrRHI"
      },
      "outputs": [],
      "source": [
        "high_avg_temp_1 = spark.sql(\"\"\"\n",
        "    SELECT *\n",
        "    FROM temp\n",
        "    ORDER BY AverageTemperature DESC\n",
        "    LIMIT 1\n",
        "\"\"\").first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql9Q5b25rZpd",
        "outputId": "f2e522a3-4b0b-4188-b205-c58278c7517d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bahrain\n",
            "2012\n"
          ]
        }
      ],
      "source": [
        "country = high_avg_temp_1[\"Country\"]\n",
        "print(country)\n",
        "\n",
        "year = high_avg_temp_1['dt'].year\n",
        "print(year)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "_CJKi9-2rk2V"
      },
      "outputs": [],
      "source": [
        "high_avg_temp_2 = temp.select(\"dt\", \"Country\") \\\n",
        "    .orderBy(col(\"AverageTemperature\").desc()) \\\n",
        "    .first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKIA5jkOr7Or",
        "outputId": "1945436d-defe-4a51-8754-b0b5cf2bdf40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bahrain\n",
            "2012\n"
          ]
        }
      ],
      "source": [
        "country = high_avg_temp_2[\"Country\"]\n",
        "print(country)\n",
        "\n",
        "year = high_avg_temp_2['dt'].year\n",
        "print(year)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1RmMw1Yr-u1"
      },
      "source": [
        "**b**. Analyze the data by country over the years, and name which are the top 10 countries with the biggest change in average temperature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgtVQphdsBhp",
        "outputId": "9ebf035b-4c2a-40a9-ebf1-4f8a635bd12b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+------------------+\n",
            "|       Country|        TempChange|\n",
            "+--------------+------------------+\n",
            "|        Canada|            43.532|\n",
            "|       Finland|            40.332|\n",
            "|       Belarus|            39.338|\n",
            "|       Estonia|38.882999999999996|\n",
            "|     Greenland|            37.997|\n",
            "|       Denmark|37.528999999999996|\n",
            "|       Armenia|            35.566|\n",
            "|          Asia|33.992000000000004|\n",
            "|       Hungary|            33.766|\n",
            "|Czech Republic|            33.708|\n",
            "+--------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "avg_temp_change = temp.filter(temp[\"AverageTemperature\"].isNotNull()) \\\n",
        "    .groupBy(\"Country\") \\\n",
        "    .agg(max(\"AverageTemperature\").alias(\"MaxTemp\"), min(\"AverageTemperature\").alias(\"MinTemp\")) \\\n",
        "    .withColumn(\"TempChange\", col(\"MaxTemp\") - col(\"MinTemp\"))\n",
        "\n",
        "countries = avg_temp_change.select(\"Country\", \"TempChange\") \\\n",
        "    .orderBy(col(\"TempChange\").desc()) \\\n",
        "    .limit(10)\n",
        "\n",
        "countries.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNCt1KCtsyOG"
      },
      "source": [
        "**Second** **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UU05-bvvsVP5"
      },
      "outputs": [],
      "source": [
        "co2_emissions = spark.read.csv(\"/content/CO2 emissions per capita per country.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok2iYScSsnM5",
        "outputId": "3a53284d-7f27-4b94-9ae2-17feccaeca24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----+----+----+----+\n",
            "|Country Name|Country Code|       1960|       1961|       1962|       1963|       1964|       1965|       1966|       1967|       1968|       1969|       1970|       1971|       1972|       1973|       1974|       1975|       1976|       1977|       1978|       1979|       1980|       1981|       1982|       1983|       1984|       1985|       1986|       1987|       1988|       1989|       1990|       1991|       1992|       1993|       1994|       1995|       1996|       1997|       1998|       1999|       2000|       2001|       2002|       2003|       2004|       2005|       2006|       2007|       2008|       2009|       2010|       2011|       2012|       2013|       2014|2015|2016|2017|2018|\n",
            "+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----+----+----+----+\n",
            "|       Aruba|         ABW|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|2.868319392|7.235198033|10.02617921| 10.6347326|26.37450321| 26.0461298| 21.4425588|22.00078616|21.03624511|20.77193616|20.31835337|20.42681771|20.58766915|20.31156677|26.19487524|25.93402441|25.67116178|26.42045209|26.51729342|27.20070778|26.94826047|  27.895574|26.23084664|25.91583295|24.67052887| 24.5058352|13.15554169|8.351294252|8.408362638|null|null|null|null|\n",
            "| Afghanistan|         AFG|0.046059897|0.053604304|0.073764791|0.074232685|0.086292452|0.101467397|0.107636955|0.123734289| 0.11549774| 0.08682346|0.150290627|0.166042044| 0.13076385|0.136279785|0.155649444|0.168928649|0.154787206|0.182963616|0.163159571|0.168376671|0.132858608|0.151972881|0.164803883|0.203635582|0.234987713|0.297827727|0.270891121|0.271611659|0.248472561|0.235694613|0.213449805|0.187672724|0.099666474|0.089154036| 0.08003917|0.072698618|0.066044698|0.059648382| 0.05520717|0.042332597| 0.03850634|0.039002334|0.048715548|  0.0518296|0.039377828|0.052948215|0.063728472|0.085417506|0.154101422|0.241722682|0.293836994|0.412016938|0.350370581|0.315601773|0.299444991|null|null|null|null|\n",
            "|      Angola|         AGO|0.097471604|0.079038085|0.201289076|0.192534735|0.201003361|0.191528411|0.246412785|0.154911578|0.256315998|0.419550564|0.528697988|0.492302233|0.635214721|0.670624323|0.652023363|0.574693143|0.415850303|0.434755038|0.646179204|0.636944237|0.598717343|0.571201904|0.485251545|0.515071547|0.487395694|0.443121443|0.426768722|  0.5184278|0.445557344|0.423524277|0.420284254|0.405450105|0.400678653|0.430889258|0.281092579|0.769173426|0.712306341|0.489209377| 0.47137391|0.574083595|0.580352661|0.573047493|0.720768849|0.497975073|0.996165478|0.979740026|1.098883898|1.197843982|1.181526759|1.232494515|1.243405585|1.252789255|1.330843018|1.254617218|1.291328315|null|null|null|null|\n",
            "|     Albania|         ALB|1.258194928|1.374186047|1.439955964|1.181681144| 1.11174196|1.166099043|1.333055465|  1.3637463|1.519551277|1.558967572|1.753239905|1.989497923|2.515914398|2.303897417|1.849006691|1.910633637|2.013584562|2.275876391| 2.53062504|2.898208518| 1.93505831|2.693023914|2.624856785|2.683239919|2.694291374|2.658015382|2.665356221|2.414060815|2.331598531|2.783243075|1.678106654|  1.3122126|0.774724911|0.723790292|0.600203708|0.654537133|0.636625313| 0.49036506|0.560271437|0.960164412|0.978174681|1.053304176|1.229540709|1.412697196|1.376212735|1.412498211|1.302576367|1.322334855|1.484311139|1.495600199|1.578573584|1.803714725|1.692908325|1.749211079|1.978763312|null|null|null|null|\n",
            "|     Andorra|         AND|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|7.467335669|7.182456636|6.912053389|6.736054846|6.494200424|6.662051684|7.065071473|7.239712718|7.660783886|7.975454404|8.019284294|7.786950001|7.590615141|7.315760706|7.358624941|7.299871936|6.746218716|6.519465912|6.427886622|6.121652341|6.122594702|5.867129945|5.916596911|5.900752587|5.832169951|null|null|null|null|\n",
            "+------------+------------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+----+----+----+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "co2_emissions.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmRQQMCJs9RD",
        "outputId": "455bbf37-6560-428d-fa97-0142ec6918f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+----+------------------------+\n",
            "|Country Name|Country Code|Year|CO2_Emissions_per_capita|\n",
            "+------------+------------+----+------------------------+\n",
            "|       Aruba|         ABW|1960|                     NaN|\n",
            "| Afghanistan|         AFG|1960|             0.046059897|\n",
            "|      Angola|         AGO|1960|             0.097471604|\n",
            "|     Albania|         ALB|1960|             1.258194928|\n",
            "|     Andorra|         AND|1960|                     NaN|\n",
            "+------------+------------+----+------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filtered_co2_emissions = co2_emissions.select(\"Country Name\", \"Country Code\", *[col(str(year)).alias(f\"`{year}`\") for year in range(1960, 2015)])\n",
        "\n",
        "df = filtered_co2_emissions.toPandas()\n",
        "\n",
        "trans_df = df.melt(id_vars=[\"Country Name\", \"Country Code\"], var_name=\"Year\", value_name=\"CO2_Emissions_per_capita\")\n",
        "\n",
        "transposed_co2_emissions = spark.createDataFrame(trans_df)\n",
        "transposed_co2_emissions = transposed_co2_emissions.withColumn(\"Year\", regexp_replace(\"Year\", \"`\", \"\"))\n",
        "\n",
        "transposed_co2_emissions.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "EdOg7KpvtcGI"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import year\n",
        "\n",
        "temp_data = temp.withColumn(\"Year\", year(\"dt\"))\n",
        "\n",
        "temp_filtered = temp_data.filter((year(temp_data[\"dt\"]) >= 1960) & (year(temp_data[\"dt\"]) <= 2014))\n",
        "\n",
        "merged_dataset = transposed_co2_emissions.join(temp_filtered, (transposed_co2_emissions[\"Country Name\"] == temp_filtered[\"Country\"]) & (transposed_co2_emissions[\"Year\"] == year(temp_filtered[\"dt\"])), \"inner\").drop(temp_filtered[\"Country\"]).drop(temp_filtered[\"dt\"])\n",
        "merged_dataset = merged_dataset.drop(transposed_co2_emissions[\"Year\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5bEkbDE6N9R",
        "outputId": "ec269823-a86b-4c1d-b36c-0f9e31557f0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+------------------------+------------------+-----------------------------+----+\n",
            "|Country Name|Country Code|CO2_Emissions_per_capita|AverageTemperature|AverageTemperatureUncertainty|Year|\n",
            "+------------+------------+------------------------+------------------+-----------------------------+----+\n",
            "|       Aruba|         ABW|                     NaN|            26.629|                        0.255|1960|\n",
            "|       Aruba|         ABW|                     NaN|            28.465|                        0.273|1960|\n",
            "|       Aruba|         ABW|                     NaN|28.883000000000003|          0.34600000000000003|1960|\n",
            "|       Aruba|         ABW|                     NaN|29.430999999999997|                        0.575|1960|\n",
            "|       Aruba|         ABW|                     NaN|            29.267|                        0.613|1960|\n",
            "+------------+------------+------------------------+------------------+-----------------------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "merged_dataset.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_8jfVJ96RtQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWa3kXTz6SDW",
        "outputId": "bcbe4952-294a-4224-dd65-c5e77b4aaa60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Correlation between CO2 emissions and temperature change: -0.41632955285479006\n"
          ]
        }
      ],
      "source": [
        "valid_dataset = merged_dataset.filter(~isnan(\"CO2_Emissions_per_capita\") & ~isnan(\"AverageTemperature\"))\n",
        "\n",
        "correlation_co2_temp = valid_dataset.select(corr(\"CO2_Emissions_per_capita\", \"AverageTemperature\")).first()[0]\n",
        "\n",
        "print(\"Correlation between CO2 emissions and temperature change:\", correlation_co2_temp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
